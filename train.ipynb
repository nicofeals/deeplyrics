{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2PweqZNlb_Q",
        "outputId": "3761a7a2-0eda-4b61-ad48-6363a28161ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "file = \"merged_lyrics_metalcore.txt\"\n",
        "file_augmented = \"merged_lyrics_metalcore_augmented.txt\"\n",
        "lyrics = open(file, \"r\").read() + open(file_augmented, \"r\").read()\n",
        "\n",
        "print('Length of text: {} characters'.format(len(lyrics)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2033922 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdpkN6qZlwQN",
        "outputId": "b7a585ff-62e2-4a7d-f3b3-de53798f3bbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chars = sorted(list(set(lyrics)))\n",
        "print('{} unique characters'.format(len(chars)))\n",
        "\n",
        "char2idx = dict((c,i) for i, c in enumerate(chars))\n",
        "idx2char = np.array(chars)\n",
        "print(char2idx)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 unique characters\n",
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '*': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '?': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E9Cv9wVlzJM",
        "outputId": "aa9c7749-2d17-48cb-97e2-9f0f48c24918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lyrics_as_int = np.array([char2idx[c] for c in lyrics])\n",
        "for char in char2idx:\n",
        "    print('{}: {}'.format(repr(char), char2idx[char]))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n': 0\n",
            "' ': 1\n",
            "'!': 2\n",
            "'\"': 3\n",
            "'&': 4\n",
            "\"'\": 5\n",
            "'*': 6\n",
            "',': 7\n",
            "'-': 8\n",
            "'.': 9\n",
            "'/': 10\n",
            "'0': 11\n",
            "'1': 12\n",
            "'2': 13\n",
            "'3': 14\n",
            "'4': 15\n",
            "'5': 16\n",
            "'6': 17\n",
            "'7': 18\n",
            "'8': 19\n",
            "'9': 20\n",
            "':': 21\n",
            "';': 22\n",
            "'?': 23\n",
            "'a': 24\n",
            "'b': 25\n",
            "'c': 26\n",
            "'d': 27\n",
            "'e': 28\n",
            "'f': 29\n",
            "'g': 30\n",
            "'h': 31\n",
            "'i': 32\n",
            "'j': 33\n",
            "'k': 34\n",
            "'l': 35\n",
            "'m': 36\n",
            "'n': 37\n",
            "'o': 38\n",
            "'p': 39\n",
            "'q': 40\n",
            "'r': 41\n",
            "'s': 42\n",
            "'t': 43\n",
            "'u': 44\n",
            "'v': 45\n",
            "'w': 46\n",
            "'x': 47\n",
            "'y': 48\n",
            "'z': 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZIbvjBsorDt"
      },
      "source": [
        "seq_length = 120\n",
        "examples_per_epoch = len(lyrics)//seq_length\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(lyrics_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO3vyNZ1o4Du"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJkssIH5o6aB",
        "outputId": "13dafee6-e1fd-458d-811b-3a5864d8fd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"bottle up the pain\\nlet it rain and wash away\\nlet it carry you to a better place\\ndon't forget my name\\nmy mind often wonde\"\n",
            "Target data: \"ottle up the pain\\nlet it rain and wash away\\nlet it carry you to a better place\\ndon't forget my name\\nmy mind often wonder\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjKAlb1pD1F",
        "outputId": "7c4382e4-1564-4d2c-e280-4a9604feeb1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEWSTuRepIN7"
      },
      "source": [
        "vocab_size = len(chars)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful=True):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=stateful,\n",
        "                            recurrent_activation=\"sigmoid\",\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=stateful,\n",
        "                            recurrent_activation=\"sigmoid\",\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ], name=\"LSTM\")\n",
        "    return model"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns_M-Qd2pOdk",
        "outputId": "d4e0797e-f28a-410b-dde9-f8fc0319f9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size=vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  stateful=False,\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "dataset"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"LSTM\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (128, None, 256)          12800     \n",
            "_________________________________________________________________\n",
            "gru_18 (GRU)                 (128, None, 1024)         3938304   \n",
            "_________________________________________________________________\n",
            "gru_19 (GRU)                 (128, None, 1024)         6297600   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (128, None, 50)           51250     \n",
            "=================================================================\n",
            "Total params: 10,299,954\n",
            "Trainable params: 10,299,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBy1sA21pSXO",
        "outputId": "0b907cf4-7301-4f50-822e-08f19ed7074d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Comment this cell to avoid training\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=loss)\n",
        "checkpoint_callbacks = [\n",
        "  tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='best_weight.h5',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor=\"loss\",\n",
        "  ),\n",
        "  tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss', min_delta=0, patience=0, verbose=0, mode='auto',\n",
        "  ),\n",
        "]\n",
        "\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=checkpoint_callbacks)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 40s 307ms/step - loss: 2.6071\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 40s 302ms/step - loss: 1.7237\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 1.4076\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 1.2498\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 1.1387\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 1.0336\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 40s 303ms/step - loss: 0.9239\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 40s 303ms/step - loss: 0.8037\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 0.6767\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 0.5523\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 0.4405\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 0.3490\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 0.2799\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 40s 302ms/step - loss: 0.2311\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 0.1966\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 0.1740\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 40s 304ms/step - loss: 0.1599\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 40s 306ms/step - loss: 0.1502\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 0.1436\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 40s 305ms/step - loss: 0.1423\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 40s 303ms/step - loss: 0.1404\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 40s 303ms/step - loss: 0.1414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2dbepbGzZop",
        "outputId": "6242aeb0-8749-41d9-e1dd-f44f71af43d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_predict = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model_predict.load_weights('best_weight.h5')\n",
        "model_predict.build(tf.TensorShape([1, None]))\n",
        "model_predict.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"LSTM\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (1, None, 256)            12800     \n",
            "_________________________________________________________________\n",
            "gru_20 (GRU)                 (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "gru_21 (GRU)                 (1, None, 1024)           6297600   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 50)             51250     \n",
            "=================================================================\n",
            "Total params: 10,299,954\n",
            "Trainable params: 10,299,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk_J3rM0xL5z"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model.predict(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XGAKCe3yJwO",
        "outputId": "4a1f56f3-942f-4a78-ffa0-c6392a60276f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "generate_text(model_predict, \"i'm sorry\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i'm sorry that i can't get out of bed\\ni'm sorry that my head's always a mess\\ni'll smoke until i fall in shade, a falling in a bo-blay of intent\\ni am certain of this\\nit will never be the same\\nno, never, ah\\nyou will never be the same\\nno, never, ah\\nyou will never be the same\\nno, never, ah\\nyou will never be the same\\nno, never, ah\\nyou will never be the guy\\nand ableat to so time are too little\\nto be the one remembers?\\ni won't let you take my name.\\n\\na suffering religion, judgement as you stand up at shame?\\n\\ni thought i wanted legacy, i thought i wanna be\\ni'm still am a messengies that everybody gets high, everybody gets low\\nlife can be such overdose\\n\\nlease ever seet your name\\ni'm the heart of your lie, we're play away\\ntheir greed in a place and put our time here to reptifut when they try to have made the night my home\\nso can you feel the ebbrace burn and start again?\\ncan we start again?\\ncan we start again?\\ncan we start again?\\ncan we start again?\\ncan we start again?\\ncan we start again?\\ncan we start aga\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    }
  ]
}